{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824cee07",
   "metadata": {},
   "source": [
    "# 提取四个数据集的test样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reason csv files\n",
    "mosi_label_csv_path = '../datasets/mosi_reason.csv'\n",
    "mosei_label_csv_path = '../datasets/mosei_reason.csv'\n",
    "sims_label_csv_path = '../datasets/sims_reason.csv'\n",
    "sims2_label_csv_path = '../datasets/sims2_reason.csv'\n",
    "\n",
    "\n",
    "def extract_test_samples(label_csv_path, dataset_name='mosi'):\n",
    "    df = pd.read_csv(label_csv_path)\n",
    "    test_samples = df[df['mode'] == 'test']\n",
    "    print(f\"Extracted {len(test_samples)} test samples from {label_csv_path}\")\n",
    "    test_samples.to_csv(f\"./{dataset_name}_test.csv\", index=False)\n",
    "\n",
    "\n",
    "extract_test_samples(mosi_label_csv_path, 'mosi')\n",
    "extract_test_samples(mosei_label_csv_path, 'mosei')\n",
    "extract_test_samples(sims_label_csv_path, 'sims')\n",
    "extract_test_samples(sims2_label_csv_path, 'sims2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class MetricsTop():\n",
    "    def __init__(self):\n",
    "        self.metrics_dict = {\n",
    "            'MOSI': self.__eval_mosi_regression,\n",
    "            'MOSEI': self.__eval_mosei_regression,\n",
    "            'SIMS': self.__eval_sims_regression,\n",
    "            'SIMS2': self.__eval_sims2_regression\n",
    "        }\n",
    "    \n",
    "\n",
    "    def __multiclass_acc(self, y_pred, y_true):\n",
    "        return np.sum(np.round(y_pred) == np.round(y_true)) / float(len(y_true))   \n",
    "\n",
    "    def __eval_mosei_regression(self, y_pred, y_true, exclude_zero=False):\n",
    "        test_preds = y_pred.view(-1).cpu().detach().numpy()  \n",
    "        test_truth = y_true.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        test_preds_a7 = np.clip(test_preds, a_min=-3., a_max=3.)\n",
    "        test_truth_a7 = np.clip(test_truth, a_min=-3., a_max=3.)\n",
    "        test_preds_a5 = np.clip(test_preds, a_min=-2., a_max=2.)\n",
    "        test_truth_a5 = np.clip(test_truth, a_min=-2., a_max=2.)\n",
    "        test_preds_a3 = np.clip(test_preds, a_min=-1., a_max=1.)\n",
    "        test_truth_a3 = np.clip(test_truth, a_min=-1., a_max=1.)\n",
    "\n",
    "\n",
    "        mae = np.mean(np.absolute(test_preds - test_truth))   \n",
    "        corr = np.corrcoef(test_preds, test_truth)[0][1]  \n",
    "        mult_a7 = self.__multiclass_acc(test_preds_a7, test_truth_a7)\n",
    "        mult_a5 = self.__multiclass_acc(test_preds_a5, test_truth_a5)\n",
    "        mult_a3 = self.__multiclass_acc(test_preds_a3, test_truth_a3)\n",
    "        \n",
    "        non_zeros = np.array([i for i, e in enumerate(test_truth) if e != 0])\n",
    "        non_zeros_binary_truth = (test_truth[non_zeros] > 0)\n",
    "        non_zeros_binary_preds = (test_preds[non_zeros] > 0)\n",
    "\n",
    "        non_zeros_acc2 = accuracy_score(non_zeros_binary_preds, non_zeros_binary_truth)\n",
    "        non_zeros_f1_score = f1_score(non_zeros_binary_preds, non_zeros_binary_truth, average='weighted')\n",
    "\n",
    "        binary_truth = (test_truth >= 0)\n",
    "        binary_preds = (test_preds >= 0)\n",
    "        acc2 = accuracy_score(binary_preds, binary_truth)\n",
    "        f_score = f1_score(binary_preds, binary_truth, average='weighted')\n",
    "        \n",
    "        eval_results = {\n",
    "            \"Has0_acc_2\":  round(acc2, 4),  # 保留四位小数\n",
    "            \"Has0_F1_score\": round(f_score, 4),\n",
    "            \"Non0_acc_2\":  round(non_zeros_acc2, 4),\n",
    "            \"Non0_F1_score\": round(non_zeros_f1_score, 4),\n",
    "            \"Mult_acc_3\": round(mult_a3, 4),\n",
    "            \"Mult_acc_5\": round(mult_a5, 4),\n",
    "            \"Mult_acc_7\": round(mult_a7, 4),\n",
    "            \"MAE\": round(mae, 4),\n",
    "            \"Corr\": round(corr, 4)\n",
    "        }\n",
    "        return eval_results\n",
    "\n",
    "\n",
    "    def __eval_mosi_regression(self, y_pred, y_true):\n",
    "        return self.__eval_mosei_regression(y_pred, y_true)\n",
    "\n",
    "    def __eval_sims_regression(self, y_pred, y_true):\n",
    "        test_preds = y_pred.view(-1).cpu().detach().numpy()\n",
    "        test_truth = y_true.view(-1).cpu().detach().numpy()\n",
    "        test_preds = np.clip(test_preds, a_min=-1., a_max=1.)\n",
    "        test_truth = np.clip(test_truth, a_min=-1., a_max=1.)\n",
    "\n",
    "        # two classes{[-1.0, 0.0], (0.0, 1.0]}\n",
    "        ms_2 = [-1.01, 0.0, 1.01]\n",
    "        test_preds_a2 = test_preds.copy()\n",
    "        test_truth_a2 = test_truth.copy()\n",
    "        for i in range(2):\n",
    "            test_preds_a2[np.logical_and(test_preds > ms_2[i], test_preds <= ms_2[i+1])] = i\n",
    "        for i in range(2):\n",
    "            test_truth_a2[np.logical_and(test_truth > ms_2[i], test_truth <= ms_2[i+1])] = i\n",
    "\n",
    "        # three classes{[-1.0, -0.1], (-0.1, 0.1], (0.1, 1.0]}\n",
    "        ms_3 = [-1.01, -0.1, 0.1, 1.01]\n",
    "        test_preds_a3 = test_preds.copy()\n",
    "        test_truth_a3 = test_truth.copy()\n",
    "        for i in range(3):\n",
    "            test_preds_a3[np.logical_and(test_preds > ms_3[i], test_preds <= ms_3[i+1])] = i\n",
    "        for i in range(3):\n",
    "            test_truth_a3[np.logical_and(test_truth > ms_3[i], test_truth <= ms_3[i+1])] = i\n",
    "        \n",
    "        # five classes{[-1.0, -0.7], (-0.7, -0.1], (-0.1, 0.1], (0.1, 0.7], (0.7, 1.0]}\n",
    "        ms_5 = [-1.01, -0.7, -0.1, 0.1, 0.7, 1.01]\n",
    "        test_preds_a5 = test_preds.copy()\n",
    "        test_truth_a5 = test_truth.copy()\n",
    "        for i in range(5):\n",
    "            test_preds_a5[np.logical_and(test_preds > ms_5[i], test_preds <= ms_5[i+1])] = i\n",
    "        for i in range(5):\n",
    "            test_truth_a5[np.logical_and(test_truth > ms_5[i], test_truth <= ms_5[i+1])] = i\n",
    " \n",
    "        mae = np.mean(np.absolute(test_preds - test_truth))   # Average L1 distance between preds and truths\n",
    "        corr = np.corrcoef(test_preds, test_truth)[0][1]\n",
    "        mult_a2 = self.__multiclass_acc(test_preds_a2, test_truth_a2)\n",
    "        mult_a3 = self.__multiclass_acc(test_preds_a3, test_truth_a3)\n",
    "        mult_a5 = self.__multiclass_acc(test_preds_a5, test_truth_a5)\n",
    "        f_score = f1_score(test_preds_a2, test_truth_a2, average='weighted')\n",
    "\n",
    "        eval_results = {\n",
    "            \"Mult_acc_2\": round(mult_a2, 4),\n",
    "            \"Mult_acc_3\": round(mult_a3, 4),\n",
    "            \"Mult_acc_5\": round(mult_a5, 4),\n",
    "            \"F1_score\": round(f_score, 4),\n",
    "            \"MAE\": round(mae, 4),\n",
    "            \"Corr\": round(corr, 4)\n",
    "        }\n",
    "\n",
    "        return eval_results\n",
    "    \n",
    "    def __eval_sims2_regression(self, y_pred, y_true):\n",
    "        return self.__eval_sims_regression(y_pred, y_true)\n",
    "    \n",
    "    def getMetics(self, datasetName):\n",
    "        return self.metrics_dict[datasetName.upper()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62c083",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb23daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "def extract_score_from_text(text):\n",
    "    match = re.search(r\"-?\\d+(\\.\\d+)?\", text)\n",
    "    if match:\n",
    "        return float(match.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "        \n",
    "\n",
    "def prompt_template(subtitle, min_score=-3.0, max_score=3.0):\n",
    "        \n",
    "    prompt = (\n",
    "        \"You are an expert in emotional analysis. Your task is to evaluate the emotional polarity intensity of a person in a video clip.\\n\\n\"\n",
    "        \"Based on the video frames, please consider the following aspects:\\n\"\n",
    "        \"- Facial expressions and body language\\n\"\n",
    "        \"- Speech rate, tone, and volume\\n\"\n",
    "        \"- The content of the spoken subtitle\\n\\n\"\n",
    "\n",
    "        f\"Then, integrate these modalities to assign a single emotional polarity score between {min_score} and {max_score}:\\n\"\n",
    "        f\"- Positive values (e.g., 0.1 to {max_score}) indicate increasingly positive emotions\\n\"\n",
    "        f\"- Negative values (e.g., -0.1 to {min_score}) indicate increasingly negative emotions\\n\"\n",
    "        \"- Values near 0 (e.g., -0.1, 0.0, 0.1) reflect neutral or weak emotional tone\\n\\n\"\n",
    "\n",
    "        f\"**Important requirement: Only output the final sentiment polarity score (e.g., {max_score-0.8}), do not output any reasoning process or explanation.**\\n\\n\"\n",
    "\n",
    "        \"Now evaluate the following subtitle:\\n\"\n",
    "        f'\"{subtitle}\"\\n\\n'\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "prompt_drbug = prompt_template(\"BUT I CAN SAFELY ASSURE YOU THAT EVEN IF THEY DIDN’T IT WOULD STILL BE A HIT\", -3.0, 3.0)\n",
    "print(prompt_drbug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb22586",
   "metadata": {},
   "source": [
    "# qwen2.5-vl-32b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-xxx\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "def encode_video(video_path):\n",
    "    with open(video_path, \"rb\") as video_file:\n",
    "        return base64.b64encode(video_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def QWen2_5_vl_32b_instruct_pridictor(video_path, question):\n",
    "\n",
    "    base64_video = encode_video(video_path)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen2.5-vl-32b-instruct\",  \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\":\"text\",\"text\": \"You are a helpful assistant.\"}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"video_url\",\n",
    "                        \"video_url\": {\"url\": f\"data:video/mp4;base64,{base64_video}\"},\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        extra_body={\"enable_thinking\": False},\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QWen2_5_vl_32b_instruct_pridictor_with_subtitle(subtitle, min_score=-3.0, max_score=3.0):\n",
    "    \"\"\"\n",
    "    使用QWen2.5 VL 32B模型进行视频情感分析，直接传入字幕文本。\n",
    "    \"\"\"\n",
    "\n",
    "    question = (\n",
    "        \"You are an expert in emotional analysis. Your task is to evaluate the emotional polarity intensity of a person in a video clip.\\n\\n\"\n",
    "        f\"Based on the subtitle, please integrate these information to assign a single emotional polarity score between {min_score} and {max_score}:\\n\"\n",
    "        f\"- Positive values (e.g., 0.1 to {max_score}) indicate increasingly positive emotions\\n\"\n",
    "        f\"- Negative values (e.g., -0.1 to {min_score}) indicate increasingly negative emotions\\n\"\n",
    "        \"- Values near 0 (e.g., -0.1, 0.0, 0.1) reflect neutral or weak emotional tone\\n\\n\"\n",
    "\n",
    "        f\"**Important requirement: Only output the final sentiment polarity score (e.g., {max_score-0.8}), do not output any reasoning process or explanation.**\\n\\n\"\n",
    "\n",
    "        \"Now evaluate the following subtitle:\\n\"\n",
    "        f'\"{subtitle}\"\\n\\n'\n",
    "    )\n",
    "    # print(question)\n",
    "\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        # model=\"qwen-plus\",\n",
    "        model=\"qwen2.5-vl-32b-instruct\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        ],\n",
    "        # Qwen3模型通过enable_thinking参数控制思考过程（开源版默认True，商业版默认False）\n",
    "        # 使用Qwen3开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n",
    "        # extra_body={\"enable_thinking\": False},\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd8b14",
   "metadata": {},
   "source": [
    "### predect for four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0585b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_csv_path, video_root, dataSet_name):\n",
    "\n",
    "    output_csv_path = f\"./{dataSet_name}_test_with_scores.csv\"\n",
    "\n",
    "    if 'sims' in dataSet_name:\n",
    "        min_score = -1.0\n",
    "        max_score = 1.0\n",
    "    else:\n",
    "        min_score = -3.0\n",
    "        max_score = 3.0\n",
    "\n",
    "    data = pd.read_csv(data_csv_path)\n",
    "\n",
    "\n",
    "    sample_pbar = tqdm(data.iterrows(), total=len(data), desc=f\"Processing {dataSet_name} samples\")\n",
    "    for index, row in sample_pbar:\n",
    "        video_path = os.path.join(video_root, row['mp4_path'])\n",
    "        subtitle = row['text_en'] if 'sims' in dataSet_name else row['text']\n",
    "\n",
    "        question = prompt_template(subtitle, min_score, max_score)\n",
    "        # print(question)\n",
    "        \n",
    "        try:\n",
    "            response = QWen2_5_vl_32b_instruct_pridictor(video_path, question)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                response = QWen2_5_vl_32b_instruct_pridictor_with_subtitle(subtitle, min_score, max_score)\n",
    "            except Exception as e:\n",
    "                response = \"0.0\"\n",
    "            \n",
    "        score = extract_score_from_text(response)\n",
    "\n",
    "        sample_pbar.set_postfix({\"index\": index, \"score\": score})\n",
    "\n",
    "        data.at[index, 'score-Qwen2.5-vl-32B'] = score\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            data.to_csv(output_csv_path, index=False)\n",
    "            \n",
    "\n",
    "    data.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8792019",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_data = predict('./sims_test.csv', '../MSA_Datasets/SIMS/Raw', 'sims')\n",
    "sims_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080932eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sims_data = pd.read_csv('./sims_test_with_scores.csv')\n",
    "print(sims_data.columns)\n",
    "print(len(sims_data))\n",
    "metrics = MetricsTop().getMetics('SIMS')\n",
    "pred = torch.tensor(sims_data['score-Qwen2.5-vl-32B'].values)\n",
    "true = torch.tensor(sims_data['label'].values)\n",
    "\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosi_data = predict('./mosi_test.csv', '../MSA_Datasets/MOSI/Raw', 'mosi')\n",
    "mosi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b518cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sims_data = pd.read_csv('./mosi_test_with_scores.csv')\n",
    "print(sims_data.columns)\n",
    "print(len(sims_data))\n",
    "metrics = MetricsTop().getMetics('MOSI')\n",
    "pred = torch.tensor(sims_data['score-Qwen2.5-vl-32B'].values)\n",
    "true = torch.tensor(sims_data['label'].values)\n",
    "\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims2_data = predict('./sims2_test.csv', '../MSA_Datasets/SIMS2/Raw', 'sims2')\n",
    "sims2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sims_data = pd.read_csv('./sims2_test_with_scores.csv')\n",
    "print(sims_data.columns)\n",
    "print(len(sims_data))\n",
    "metrics = MetricsTop().getMetics('SIMS2')\n",
    "pred = torch.tensor(sims_data['score-Qwen2.5-vl-32B'].values)\n",
    "true = torch.tensor(sims_data['label'].values)\n",
    "\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosei_data = predict('./mosei_test.csv', '../MSA_Datasets/MOSEI/Raw', 'mosei')\n",
    "mosei_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "sims_data = pd.read_csv('./mosei_test_with_scores.csv')\n",
    "print(len(sims_data))\n",
    "metrics = MetricsTop().getMetics('MOSEI')\n",
    "zero_count = (sims_data['score-Qwen2.5-vl-32B'] == 0.0).sum()\n",
    "print(f\"Number of samples with score 0.0: {zero_count}\")\n",
    "pred = torch.tensor(sims_data['score-Qwen2.5-vl-32B'].values)\n",
    "true = torch.tensor(sims_data['label'].values)\n",
    "\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5a70f",
   "metadata": {},
   "source": [
    "# GPPT-4o for mosi, sims, mosei, sims2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "\n",
    "def encode_frames(video_path, frames_num=3):\n",
    "    base64Frames = []\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print(f\"无法打开视频文件: {video_path}\")\n",
    "        return base64Frames\n",
    "\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_indices = [int(total_frames * i / (frames_num + 1)) for i in range(1, frames_num + 1)]\n",
    "\n",
    "    target_width = 320\n",
    "    target_height = 210\n",
    "\n",
    "    current_frame = 0\n",
    "    while video.isOpened():\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if current_frame in frame_indices:\n",
    "            resized_frame = cv2.resize(frame, (target_width, target_height))\n",
    "            _, buffer = cv2.imencode(\".jpg\", resized_frame)\n",
    "            base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        current_frame += 1\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    return base64Frames\n",
    "\n",
    "# 示例调用\n",
    "video_path = \"\"\n",
    "frames_num = 3\n",
    "base64Frames = encode_frames(video_path, frames_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import base64\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Image\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://xiaoai.plus/v1',\n",
    "    api_key='sk-xxx'  # replace with your API Key\n",
    ")\n",
    "\n",
    "def GPT_4o_predictor(frames, prompt):\n",
    "\n",
    "    PROMPT_MESSAGES = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "                *[{\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": 'data:image/jpeg;base64,' + frame,\n",
    "                    }\n",
    "                } for frame in frames]\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": PROMPT_MESSAGES,\n",
    "        \"max_tokens\": 100, \n",
    "    }\n",
    "    result = client.chat.completions.create(**params)\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "prompt = prompt_template(\"IN FACT I HAVE TO SAY THAT THIS WAS ONE OF THOSE OBNOXIOUS MAIN CHARACTERS IVE SEEN A LONG TIME\", -3.0, 3.0)\n",
    "print(prompt)\n",
    "base64Frames = encode_frames(video_path, frames_num=3)\n",
    "score = GPT_4o_predictor(base64Frames, prompt)\n",
    "print(f\"Extracted score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eda68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_GPT4o(data_csv_path, video_root, dataSet_name):\n",
    "\n",
    "    output_csv_path = f\"./{dataSet_name}_test_with_scores.csv\"\n",
    "\n",
    "    if 'sims' in dataSet_name:\n",
    "        min_score = -1.0\n",
    "        max_score = 1.0\n",
    "    else:\n",
    "        min_score = -3.0\n",
    "        max_score = 3.0\n",
    "\n",
    "    data = pd.read_csv(data_csv_path)\n",
    "\n",
    "\n",
    "    sample_pbar = tqdm(data.iterrows(), total=len(data), desc=f\"Processing {dataSet_name} samples\")\n",
    "    for index, row in sample_pbar:\n",
    "        video_path = os.path.join(video_root, row['mp4_path'])\n",
    "        subtitle = row['text_en'] if 'sims' in dataSet_name else row['text']\n",
    "        base64Frames = encode_frames(video_path, frames_num=3)\n",
    "\n",
    "        question = prompt_template(subtitle, min_score, max_score)\n",
    "        # print(question)\n",
    "        \n",
    "        try:\n",
    "            response = GPT_4o_predictor(base64Frames, question)\n",
    "        except Exception as e:\n",
    "            response = \"0.0\"\n",
    "            \n",
    "        score = extract_score_from_text(response)\n",
    "\n",
    "        sample_pbar.set_postfix({\"index\": index, \"score\": score})\n",
    "\n",
    "        data.at[index, 'GPT-4o'] = score\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            data.to_csv(output_csv_path, index=False)\n",
    "            \n",
    "\n",
    "    data.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_data = predict_GPT4o('./sims_test_with_scores.csv', '../MSA_Datasets/SIMS/Raw', 'sims')\n",
    "sims_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sims_data = pd.read_csv('./sims_test_with_scores.csv')\n",
    "print(\"Number of samples: \", len(sims_data))\n",
    "zero_count = (sims_data['GPT-4o'] == 0.0).sum()\n",
    "print(f\"Number of samples with GPT-4o score 0.0: {zero_count}\")\n",
    "\n",
    "metrics = MetricsTop().getMetics('SIMS')\n",
    "pred = torch.tensor(sims_data['GPT-4o'].values)\n",
    "true = torch.tensor(sims_data['label'].values)\n",
    "\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosi_data = predict_GPT4o('./mosi_test_with_scores.csv', '../MSA_Datasets/MOSI/Raw', 'mosi')\n",
    "mosi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce40621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "mosi_data = pd.read_csv('./mosi_test_with_scores.csv')\n",
    "print(\"Number of samples: \", len(mosi_data))\n",
    "zero_count = (mosi_data['GPT-4o'] == 0.0).sum()\n",
    "print(f\"Number of samples with GPT-4o score 0.0: {zero_count}\")\n",
    "metrics = MetricsTop().getMetics('MOSI')\n",
    "pred = torch.tensor(mosi_data['GPT-4o'].values)\n",
    "true = torch.tensor(mosi_data['label'].values)\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims2_data = predict_GPT4o('./sims2_test_with_scores.csv', '../MSA_Datasets/SIMS2/Raw', 'sims2')\n",
    "sims2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sims2_data = pd.read_csv('./sims2_test_with_scores.csv')\n",
    "print(\"Number of samples: \", len(sims2_data))\n",
    "zero_count = (sims2_data['GPT-4o'] == 0.0).sum()\n",
    "print(f\"Number of samples with GPT-4o score 0.0: {zero_count}\")\n",
    "metrics = MetricsTop().getMetics('SIMS2')\n",
    "pred = torch.tensor(sims2_data['GPT-4o'].values)\n",
    "true = torch.tensor(sims2_data['label'].values)\n",
    "test_results = metrics(pred, true)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7813aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosei_data = predict_GPT4o('./mosei_test_with_scores.csv', '../MSA_Datasets/MOSEI/Raw', 'mosei')\n",
    "mosei_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
